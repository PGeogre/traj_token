# è½¨è¿¹é¢„è®­ç»ƒ+å¾®è°ƒé¡¹ç›®ä½¿ç”¨è¯´æ˜

æœ¬é¡¹ç›®å®ç°äº†è½¨è¿¹æ•°æ®çš„é¢„è®­ç»ƒ+å¾®è°ƒå®Œæ•´æµç¨‹ï¼Œç”¨äºèˆ¹èˆ¶åˆ†ç±»ä»»åŠ¡ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
â”œâ”€â”€ prepare_dataset.py          # åŸå§‹æ•°æ®é¢„å¤„ç†
â”œâ”€â”€ pretrain_data_processor.py  # é¢„è®­ç»ƒæ•°æ®å¤„ç†ï¼ˆMLMæ©ç ï¼‰
â”œâ”€â”€ pretrain_models.py          # é¢„è®­ç»ƒå’Œå¾®è°ƒæ¨¡å‹æ¶æ„
â”œâ”€â”€ pretrain_config.py          # é…ç½®æ–‡ä»¶
â”œâ”€â”€ train_pretrain.py           # é¢„è®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_finetune.py           # å¾®è°ƒè„šæœ¬
â”œâ”€â”€ train_model.py              # å¢å¼ºç‰ˆåŸå§‹è®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒé¢„è®­ç»ƒï¼‰
â”œâ”€â”€ run_training.sh             # ä¸€é”®è¿è¡Œè„šæœ¬
â””â”€â”€ data_loader.py              # æ•°æ®åŠ è½½å™¨
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1: ä¸€é”®è¿è¡Œå®Œæ•´æµç¨‹
```bash
# è¿è¡Œå®Œæ•´çš„é¢„è®­ç»ƒ+å¾®è°ƒæµç¨‹
./run_training.sh full
```

### æ–¹æ³•2: åˆ†æ­¥è¿è¡Œ

#### æ­¥éª¤1: é¢„è®­ç»ƒ
```bash
# åªè¿è¡Œé¢„è®­ç»ƒ
./run_training.sh pretrain

# æˆ–è€…æ‰‹åŠ¨è¿è¡Œ
python train_pretrain.py
```

#### æ­¥éª¤2: å¾®è°ƒ
```bash
# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒ
./run_training.sh finetune ../model_out/trajectory_pretrain_model/best_model

# æˆ–è€…æ‰‹åŠ¨è¿è¡Œ
python train_finetune.py --pretrained_model_path ../model_out/trajectory_pretrain_model/best_model
```

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„

### é¢„è®­ç»ƒé˜¶æ®µ (MLM)
- **ç›®æ ‡**: å­¦ä¹ è½¨è¿¹çš„ç©ºé—´-æ—¶é—´æ¨¡å¼å’Œèˆ¹èˆ¶è¡Œä¸ºç‰¹å¾
- **ä»»åŠ¡**: Masked Language Modeling (MLM)
- **æ©ç ç­–ç•¥**:
  - 40% æ¦‚ç‡æ©ç H3åœ°ç†ä½ç½®tokens
  - 30% æ¦‚ç‡æ©ç èˆ¹èˆ¶ç±»åˆ«tokens  
  - 20% æ¦‚ç‡æ©ç é€Ÿåº¦/èˆªå‘tokens
  - 10% æ¦‚ç‡éšæœºæ©ç å…¶ä»–tokens

### å¾®è°ƒé˜¶æ®µ (åˆ†ç±»)
- **ç›®æ ‡**: èˆ¹èˆ¶ç±»å‹åˆ†ç±»
- **ç­–ç•¥**: åŠ è½½é¢„è®­ç»ƒbackbone + åˆ†ç±»å¤´
- **ä¼˜åŒ–**: åˆ†å±‚å­¦ä¹ ç‡ï¼ˆbackboneç”¨è¾ƒå°LRï¼‰

## âš™ï¸ å…³é”®é…ç½®

### é¢„è®­ç»ƒé…ç½® (pretrain_config.py)
```python
class PretrainConfig:
    mask_prob = 0.15          # æ€»ä½“æ©ç æ¦‚ç‡
    batch_size = 8            # é¢„è®­ç»ƒbatch size
    learning_rate = 5e-5      # é¢„è®­ç»ƒå­¦ä¹ ç‡
    num_epochs = 50           # é¢„è®­ç»ƒè½®æ•°
    early_stopping_patience = 5
```

### å¾®è°ƒé…ç½®
```python
class FineTuneConfig:
    batch_size = 4            # å¾®è°ƒbatch size
    learning_rate = 2e-5      # å¾®è°ƒå­¦ä¹ ç‡ï¼ˆæ›´å°ï¼‰
    num_epochs = 20           # å¾®è°ƒè½®æ•°ï¼ˆæ›´å°‘ï¼‰
    freeze_backbone_epochs = 0 # å†»ç»“backboneè½®æ•°
```

## ğŸ“Š è®­ç»ƒç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®æŒ‡æ ‡ï¼š

### é¢„è®­ç»ƒç›‘æ§
- MLM Loss: æ©ç é¢„æµ‹æŸå¤±
- MLM Accuracy: æ©ç é¢„æµ‹å‡†ç¡®ç‡
- å­¦ä¹ ç‡å˜åŒ–

### å¾®è°ƒç›‘æ§  
- åˆ†ç±»Losså’ŒAccuracy
- å„ç±»åˆ«çš„F1-score
- æ··æ·†çŸ©é˜µ

## ğŸ—‚ï¸ è¾“å‡ºæ–‡ä»¶

```
../model_out/
â”œâ”€â”€ trajectory_pretrain_model/
â”‚   â”œâ”€â”€ best_model/           # æœ€ä½³é¢„è®­ç»ƒæ¨¡å‹
â”‚   â”œâ”€â”€ final_model/          # æœ€ç»ˆé¢„è®­ç»ƒæ¨¡å‹  
â”‚   â””â”€â”€ training_log.jsonl    # é¢„è®­ç»ƒæ—¥å¿—
â”œâ”€â”€ trajectory_finetune_model/
â”‚   â”œâ”€â”€ best_model/           # æœ€ä½³å¾®è°ƒæ¨¡å‹ï¼ˆç”¨äºæ¨ç†ï¼‰
â”‚   â”œâ”€â”€ final_model/          # æœ€ç»ˆå¾®è°ƒæ¨¡å‹
â”‚   â”œâ”€â”€ finetune_log.jsonl    # å¾®è°ƒæ—¥å¿—
â”‚   â””â”€â”€ validation_report.json # éªŒè¯æŠ¥å‘Š
â””â”€â”€ logs/                     # è®­ç»ƒæ—¥å¿—
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### åˆ†å¸ƒå¼è®­ç»ƒ
```bash
# è‡ªåŠ¨æ£€æµ‹GPUæ•°é‡å¹¶ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
python -m torch.distributed.launch --nproc_per_node=4 train_pretrain.py
python -m torch.distributed.launch --nproc_per_node=4 train_finetune.py
```

### è‡ªå®šä¹‰é…ç½®
```python
# ä¿®æ”¹ pretrain_config.py ä¸­çš„é…ç½®
config = PretrainConfig()
config.mask_prob = 0.20           # å¢åŠ æ©ç æ¦‚ç‡
config.h3_mask_prob = 0.5         # æ›´å¤šH3æ©ç 
config.learning_rate = 1e-4       # è°ƒæ•´å­¦ä¹ ç‡
```

### ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ ‡å‡†è®­ç»ƒ
```bash
# ä½¿ç”¨å¢å¼ºç‰ˆtrain_model.py
python train_model.py --use_pretrained --pretrained_model_path ../model_out/trajectory_pretrain_model/best_model
```

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

ç›¸æ¯”ç›´æ¥è®­ç»ƒï¼Œé¢„è®­ç»ƒ+å¾®è°ƒåº”è¯¥èƒ½å¸¦æ¥ï¼š

1. **æ›´å¥½çš„æ”¶æ•›**: æ›´å¿«è¾¾åˆ°è¾ƒé«˜ç²¾åº¦
2. **æ›´å¼ºçš„æ³›åŒ–**: åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°æ›´ç¨³å®š  
3. **æ•°æ®æ•ˆç‡**: å……åˆ†åˆ©ç”¨æ‰€æœ‰è½¨è¿¹æ•°æ®
4. **ç‰¹å¾å­¦ä¹ **: æ›´å¥½ç†è§£è½¨è¿¹çš„ç©ºé—´-æ—¶é—´æ¨¡å¼

## ğŸ› å¸¸è§é—®é¢˜

### Q: é¢„è®­ç»ƒæ•°æ®ç”Ÿæˆå¤±è´¥
A: æ£€æŸ¥åŸå§‹æ•°æ®è·¯å¾„ï¼Œç¡®ä¿ `train_dataset_nj.jsonl` å­˜åœ¨

### Q: GPUå†…å­˜ä¸è¶³
A: å‡å° `batch_size` æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

### Q: åˆ†å¸ƒå¼è®­ç»ƒå¤±è´¥
A: æ£€æŸ¥ç«¯å£å ç”¨ï¼Œå¯ä¿®æ”¹ `--master_port` å‚æ•°

### Q: æ¨¡å‹åŠ è½½å¤±è´¥
A: ç¡®ä¿é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„æ­£ç¡®ï¼Œæ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§

## ğŸ“ æ—¥å¿—åˆ†æ

### æŸ¥çœ‹è®­ç»ƒè¿›åº¦
```bash
# å®æ—¶æŸ¥çœ‹é¢„è®­ç»ƒæ—¥å¿—
tail -f ../model_out/logs/pretrain_*.log

# æŸ¥çœ‹å¾®è°ƒæ—¥å¿—
tail -f ../model_out/logs/finetune_*.log
```

### è§£æè®­ç»ƒæŒ‡æ ‡
```python
import json

# è§£æé¢„è®­ç»ƒæ—¥å¿—
with open('../model_out/trajectory_pretrain_model/training_log.jsonl') as f:
    logs = [json.loads(line) for line in f]
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {min(log['val_loss'] for log in logs)}")

# æŸ¥çœ‹å¾®è°ƒç»“æœ
with open('../model_out/trajectory_finetune_model/best_model/validation_report.json') as f:
    report = json.load(f)
    print(f"æ•´ä½“å‡†ç¡®ç‡: {report['accuracy']:.4f}")
```

---

ğŸ¯ **å¿«é€Ÿå¼€å§‹å»ºè®®**: ç›´æ¥è¿è¡Œ `./run_training.sh full` ä½“éªŒå®Œæ•´æµç¨‹ï¼