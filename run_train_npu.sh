#!/bin/bash

# æ˜‡è…¾NPUè½¨è¿¹é¢„è®­ç»ƒ+å¾®è°ƒå®Œæ•´è®­ç»ƒè„šæœ¬
# é€‚ç”¨äºå•æœº8å¡æ˜‡è…¾NPUè®­ç»ƒç¯å¢ƒ
# ä½¿ç”¨æ–¹æ³•ï¼š
# 1. åªé¢„è®­ç»ƒï¼š./run_train_npu.sh pretrain
# 2. åªå¾®è°ƒï¼š./run_train_npu.sh finetune /path/to/pretrained/model
# 3. å®Œæ•´æµç¨‹ï¼ˆé¢„è®­ç»ƒ+å¾®è°ƒï¼‰ï¼š./run_train_npu.sh full

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "=== æ˜‡è…¾NPUè½¨è¿¹æ¨¡å‹è®­ç»ƒè„šæœ¬ ==="

# é…ç½®å‚æ•°
BASE_DIR="/home/maomao/project/token_for_nj"
OUTPUT_BASE="../model_out"
PRETRAIN_OUTPUT="$OUTPUT_BASE/trajectory_pretrain_model_npu"
FINETUNE_OUTPUT="$OUTPUT_BASE/trajectory_finetune_model_npu"
LOG_DIR="$OUTPUT_BASE/logs_npu"

# åˆ›å»ºå¿…è¦çš„ç›®å½•
mkdir -p "$LOG_DIR"

# è·å–å½“å‰æ—¶é—´æˆ³
TIMESTAMP=$(date "+%Y%m%d_%H%M%S")

# æ£€æŸ¥æ˜‡è…¾NPUç¯å¢ƒ
function check_npu_environment() {
    echo "æ£€æŸ¥æ˜‡è…¾NPUç¯å¢ƒ..."
    
    # æ£€æŸ¥npu-smiå‘½ä»¤
    if ! command -v npu-smi &> /dev/null; then
        echo "âŒ é”™è¯¯: æœªæ‰¾åˆ°npu-smiå‘½ä»¤"
        echo "è¯·ç¡®è®¤æ˜‡è…¾NPUé©±åŠ¨å’Œå·¥å…·å·²æ­£ç¡®å®‰è£…"
        exit 1
    fi
    
    # æ£€æŸ¥NPUè®¾å¤‡æ•°é‡
    NPU_COUNT=$(npu-smi info 2>/dev/null | grep "NPU" | wc -l || echo "0")
    echo "æ£€æµ‹åˆ° ${NPU_COUNT} ä¸ªNPUè®¾å¤‡"
    
    if [ ${NPU_COUNT} -lt 1 ]; then
        echo "âŒ é”™è¯¯: æ²¡æœ‰æ£€æµ‹åˆ°å¯ç”¨çš„NPUè®¾å¤‡"
        echo "è¯·æ£€æŸ¥NPUè®¾å¤‡çŠ¶æ€ï¼šnpu-smi info"
        exit 1
    fi
    
    # æ£€æŸ¥Pythonç¯å¢ƒä¸­çš„torch_npu
    if ! python3 -c "import torch_npu" 2>/dev/null; then
        echo "âŒ é”™è¯¯: æœªæ‰¾åˆ°torch_npuæ¨¡å—"
        echo "è¯·å®‰è£…æ˜‡è…¾PyTorchæ’ä»¶ï¼š"
        echo "  pip install torch_npu"
        exit 1
    fi
    
    # æ£€æŸ¥å…¶ä»–å¿…è¦çš„PythonåŒ…
    if ! python3 -c "import torch, transformers, sklearn, tqdm" 2>/dev/null; then
        echo "âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„PythonåŒ…"
        echo "è¯·å®‰è£…æ‰€éœ€ä¾èµ–ï¼š"
        echo "  pip install torch transformers scikit-learn tqdm"
        exit 1
    fi
    
    echo "âœ… æ˜‡è…¾NPUç¯å¢ƒæ£€æŸ¥é€šè¿‡"
    echo "NPUè®¾å¤‡æ•°é‡: ${NPU_COUNT}"
    
    # æ˜¾ç¤ºNPUè®¾å¤‡ä¿¡æ¯
    echo "NPUè®¾å¤‡è¯¦æƒ…:"
    npu-smi info | head -10
    
    return 0
}

# è®¾ç½®æ˜‡è…¾NPUç¯å¢ƒå˜é‡
function setup_npu_env() {
    echo "è®¾ç½®æ˜‡è…¾NPUç¯å¢ƒå˜é‡..."
    
    # æ˜‡è…¾NPUæ ¸å¿ƒç¯å¢ƒå˜é‡
    export HCCL_WHITELIST_DISABLE=1
    export HCCL_IF_IP=$(hostname -I | awk '{print $1}')
    
    # NPUä¼˜åŒ–ç›¸å…³ç¯å¢ƒå˜é‡
    export NPU_CALCULATE_DEVICE=0,1,2,3,4,5,6,7
    export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    
    # å†…å­˜å’Œæ€§èƒ½ä¼˜åŒ–
    export HCCL_BUFFSIZE=120
    export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
    
    # æ—¥å¿—çº§åˆ«ï¼ˆå¯é€‰ï¼‰
    export ASCEND_SLOG_PRINT_TO_STDOUT=1
    export ASCEND_GLOBAL_LOG_LEVEL=1
    
    echo "âœ… NPUç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ"
}

echo "æ—¶é—´: $(date)"
echo "åŸºç¡€ç›®å½•: $BASE_DIR"
echo "æ—¥å¿—ç›®å½•: $LOG_DIR"

# è¿›å…¥å·¥ä½œç›®å½•
cd "$BASE_DIR"

function run_pretraining_npu() {
    echo ""
    echo "========================="
    echo "å¼€å§‹NPUé¢„è®­ç»ƒé˜¶æ®µ"
    echo "========================="
    
    local log_file="$LOG_DIR/pretrain_npu_$TIMESTAMP.log"
    echo "é¢„è®­ç»ƒæ—¥å¿—: $log_file"
    echo "é¢„è®­ç»ƒæ¨¡å‹ä¿å­˜è·¯å¾„: $PRETRAIN_OUTPUT"
    
    # æ£€æŸ¥è®­ç»ƒè„šæœ¬æ˜¯å¦å­˜åœ¨
    if [ ! -f "train_pretrain.py" ]; then
        echo "âŒ é”™è¯¯: æœªæ‰¾åˆ°é¢„è®­ç»ƒè„šæœ¬ train_pretrain.py"
        exit 1
    fi
    
    # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„NPUè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
    local npu_count=${NPU_COUNT:-8}  # é»˜è®¤ä½¿ç”¨8ä¸ªNPU
    echo "ä½¿ç”¨ ${npu_count} ä¸ªNPUè¿›è¡Œåˆ†å¸ƒå¼é¢„è®­ç»ƒ"
    
    # ä½¿ç”¨torchrunå¯åŠ¨åˆ†å¸ƒå¼é¢„è®­ç»ƒ
    echo "å¯åŠ¨NPUåˆ†å¸ƒå¼é¢„è®­ç»ƒ..."
    torchrun \
        --standalone \
        --nnodes=1 \
        --nproc_per_node=${npu_count} \
        train_pretrain.py \
        --config_file train_config_npu.py \
        --device_type npu \
        --output_dir "$PRETRAIN_OUTPUT" \
        2>&1 | tee "$log_file"
    
    # æ£€æŸ¥é¢„è®­ç»ƒæ˜¯å¦æˆåŠŸ
    if [ -d "$PRETRAIN_OUTPUT/best_model" ] || [ -f "$PRETRAIN_OUTPUT/best_model.pt" ]; then
        echo "âœ… NPUé¢„è®­ç»ƒæˆåŠŸå®Œæˆ"
        echo "é¢„è®­ç»ƒæ¨¡å‹ä¿å­˜åœ¨: $PRETRAIN_OUTPUT"
        
        # æ˜¾ç¤ºæ¨¡å‹æ–‡ä»¶
        echo "æ¨¡å‹æ–‡ä»¶åˆ—è¡¨:"
        ls -la "$PRETRAIN_OUTPUT"/ || ls -la "$PRETRAIN_OUTPUT/best_model/" 2>/dev/null || true
    else
        echo "âŒ NPUé¢„è®­ç»ƒå¤±è´¥ï¼Œæœªæ‰¾åˆ°æ¨¡å‹è¾“å‡º"
        echo "è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶: $log_file"
        exit 1
    fi
}

function run_finetuning_npu() {
    local pretrained_path="$1"
    
    echo ""
    echo "========================="
    echo "å¼€å§‹NPUå¾®è°ƒé˜¶æ®µ"
    echo "========================="
    
    # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹æ˜¯å¦å­˜åœ¨
    if [ ! -d "$pretrained_path" ] && [ ! -f "$pretrained_path" ]; then
        echo "âŒ é”™è¯¯: é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: $pretrained_path"
        echo "è¯·å…ˆè¿è¡Œé¢„è®­ç»ƒæˆ–æä¾›æ­£ç¡®çš„é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„"
        exit 1
    fi
    
    local log_file="$LOG_DIR/finetune_npu_$TIMESTAMP.log"
    echo "å¾®è°ƒæ—¥å¿—: $log_file"
    echo "é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„: $pretrained_path"
    echo "å¾®è°ƒæ¨¡å‹ä¿å­˜è·¯å¾„: $FINETUNE_OUTPUT"
    
    # æ£€æŸ¥å¾®è°ƒè„šæœ¬æ˜¯å¦å­˜åœ¨ï¼Œä¼˜å…ˆä½¿ç”¨NPUç‰ˆæœ¬
    local finetune_script=""
    if [ -f "train_finetune_npu.py" ]; then
        finetune_script="train_finetune_npu.py"
    elif [ -f "train_model_npu.py" ]; then
        finetune_script="train_model_npu.py"
    elif [ -f "train_finetune.py" ]; then
        finetune_script="train_finetune.py"
        echo "âš ï¸  è­¦å‘Š: ä½¿ç”¨é€šç”¨å¾®è°ƒè„šæœ¬ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨é€‚é…NPU"
    else
        echo "âŒ é”™è¯¯: æœªæ‰¾åˆ°å¾®è°ƒè®­ç»ƒè„šæœ¬"
        exit 1
    fi
    
    echo "ä½¿ç”¨è®­ç»ƒè„šæœ¬: $finetune_script"
    
    # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„NPUè¿›è¡Œåˆ†å¸ƒå¼å¾®è°ƒ
    local npu_count=${NPU_COUNT:-8}
    echo "ä½¿ç”¨ ${npu_count} ä¸ªNPUè¿›è¡Œåˆ†å¸ƒå¼å¾®è°ƒ"
    
    # å¯åŠ¨NPUåˆ†å¸ƒå¼å¾®è°ƒ
    echo "å¯åŠ¨NPUåˆ†å¸ƒå¼å¾®è°ƒ..."
    torchrun \
        --standalone \
        --nnodes=1 \
        --nproc_per_node=${npu_count} \
        "$finetune_script" \
        --pretrained_model_path "$pretrained_path" \
        --config_file train_config_npu.py \
        --device_type npu \
        --output_dir "$FINETUNE_OUTPUT" \
        2>&1 | tee "$log_file"
    
    # æ£€æŸ¥å¾®è°ƒæ˜¯å¦æˆåŠŸ
    if [ -d "$FINETUNE_OUTPUT/best_model" ] || [ -f "$FINETUNE_OUTPUT/best_model.pt" ]; then
        echo "âœ… NPUå¾®è°ƒæˆåŠŸå®Œæˆ"
        echo "æœ€ç»ˆæ¨¡å‹ä¿å­˜åœ¨: $FINETUNE_OUTPUT"
        
        # æ˜¾ç¤ºæ¨¡å‹æ–‡ä»¶
        echo "æ¨¡å‹æ–‡ä»¶åˆ—è¡¨:"
        ls -la "$FINETUNE_OUTPUT"/ || ls -la "$FINETUNE_OUTPUT/best_model/" 2>/dev/null || true
        
        # å¦‚æœæœ‰éªŒè¯æŠ¥å‘Šï¼Œæ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        local report_file=""
        if [ -f "$FINETUNE_OUTPUT/best_model/validation_report.json" ]; then
            report_file="$FINETUNE_OUTPUT/best_model/validation_report.json"
        elif [ -f "$FINETUNE_OUTPUT/validation_report.json" ]; then
            report_file="$FINETUNE_OUTPUT/validation_report.json"
        fi
        
        if [ -n "$report_file" ]; then
            echo ""
            echo "éªŒè¯ç»“æœæ‘˜è¦:"
            python3 -c "
import json
import sys
try:
    with open('$report_file') as f:
        report = json.load(f)
        print(f\"æ•´ä½“å‡†ç¡®ç‡: {report.get('accuracy', 'N/A'):.4f}\")
        print(f\"å®å¹³å‡F1: {report.get('macro avg', {}).get('f1-score', 'N/A'):.4f}\")
        print(f\"åŠ æƒå¹³å‡F1: {report.get('weighted avg', {}).get('f1-score', 'N/A'):.4f}\")
except Exception as e:
    print(f\"æ— æ³•è¯»å–éªŒè¯æŠ¥å‘Š: {e}\")
"
        fi
    else
        echo "âŒ NPUå¾®è°ƒå¤±è´¥ï¼Œæœªæ‰¾åˆ°æ¨¡å‹è¾“å‡º"
        echo "è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶: $log_file"
        exit 1
    fi
}

function run_distributed_training_npu() {
    local mode="$1"
    local pretrained_path="$2"
    
    echo "ä½¿ç”¨ç°æœ‰çš„NPUåˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬..."
    
    if [ -f "train_distributed_npu.sh" ]; then
        echo "å‘ç°ä¸“ç”¨çš„NPUåˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬"
        chmod +x train_distributed_npu.sh
        
        local log_file="$LOG_DIR/distributed_npu_$TIMESTAMP.log"
        
        case "$mode" in
            "pretrain")
                echo "ä½¿ç”¨NPUåˆ†å¸ƒå¼è„šæœ¬è¿›è¡Œé¢„è®­ç»ƒ"
                ./train_distributed_npu.sh 2>&1 | tee "$log_file"
                ;;
            "finetune")
                echo "ä½¿ç”¨NPUåˆ†å¸ƒå¼è„šæœ¬è¿›è¡Œå¾®è°ƒ"
                # å¯èƒ½éœ€è¦ä¿®æ”¹train_distributed_npu.shæ¥æ”¯æŒå¾®è°ƒå‚æ•°
                ./train_distributed_npu.sh 2>&1 | tee "$log_file"
                ;;
        esac
    else
        echo "æœªæ‰¾åˆ°ä¸“ç”¨NPUåˆ†å¸ƒå¼è„šæœ¬ï¼Œä½¿ç”¨å†…ç½®å‡½æ•°"
        case "$mode" in
            "pretrain")
                run_pretraining_npu
                ;;
            "finetune")
                run_finetuning_npu "$pretrained_path"
                ;;
        esac
    fi
}

function show_usage() {
    echo "ç”¨æ³•: $0 <mode> [pretrained_model_path]"
    echo ""
    echo "æ¨¡å¼:"
    echo "  pretrain              - åªè¿è¡ŒNPUé¢„è®­ç»ƒ"
    echo "  finetune <model_path> - åªè¿è¡ŒNPUå¾®è°ƒï¼Œéœ€è¦æŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„"
    echo "  full                  - è¿è¡Œå®Œæ•´NPUæµç¨‹ï¼ˆé¢„è®­ç»ƒ + å¾®è°ƒï¼‰"
    echo "  check                 - åªæ£€æŸ¥NPUç¯å¢ƒ"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  $0 check"
    echo "  $0 pretrain"
    echo "  $0 finetune ../model_out/trajectory_pretrain_model_npu/best_model"
    echo "  $0 full"
}

# ä¸»é€»è¾‘
case "$1" in
    "check")
        check_npu_environment
        setup_npu_env
        echo ""
        echo "âœ… NPUç¯å¢ƒæ£€æŸ¥å®Œæˆï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ"
        ;;
    
    "pretrain")
        check_npu_environment
        setup_npu_env
        run_pretraining_npu
        echo ""
        echo "ğŸ‰ NPUé¢„è®­ç»ƒå®Œæˆï¼"
        echo "æ¥ä¸‹æ¥å¯ä»¥è¿è¡Œå¾®è°ƒ: $0 finetune $PRETRAIN_OUTPUT/best_model"
        ;;
    
    "finetune")
        if [ -z "$2" ]; then
            echo "âŒ é”™è¯¯: å¾®è°ƒæ¨¡å¼éœ€è¦æŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„"
            show_usage
            exit 1
        fi
        check_npu_environment
        setup_npu_env
        run_finetuning_npu "$2"
        echo ""
        echo "ğŸ‰ NPUå¾®è°ƒå®Œæˆï¼"
        echo "æœ€ç»ˆæ¨¡å‹å¯ç”¨äºæ¨ç†: $FINETUNE_OUTPUT/best_model"
        ;;
    
    "full")
        echo "è¿è¡Œå®Œæ•´NPUè®­ç»ƒæµç¨‹ï¼šé¢„è®­ç»ƒ -> å¾®è°ƒ"
        check_npu_environment
        setup_npu_env
        
        # 1. é¢„è®­ç»ƒ
        run_pretraining_npu
        
        # 2. å¾®è°ƒ
        local pretrain_model_path="$PRETRAIN_OUTPUT/best_model"
        if [ ! -d "$pretrain_model_path" ] && [ -f "$PRETRAIN_OUTPUT/best_model.pt" ]; then
            pretrain_model_path="$PRETRAIN_OUTPUT/best_model.pt"
        fi
        
        run_finetuning_npu "$pretrain_model_path"
        
        echo ""
        echo "ğŸ‰ğŸ‰ğŸ‰ å®Œæ•´NPUè®­ç»ƒæµç¨‹å®Œæˆï¼"
        echo "é¢„è®­ç»ƒæ¨¡å‹: $PRETRAIN_OUTPUT"
        echo "æœ€ç»ˆå¾®è°ƒæ¨¡å‹: $FINETUNE_OUTPUT"
        ;;
    
    "")
        echo "âŒ é”™è¯¯: è¯·æŒ‡å®šè®­ç»ƒæ¨¡å¼"
        show_usage
        exit 1
        ;;
    
    *)
        echo "âŒ é”™è¯¯: æ— æ•ˆçš„æ¨¡å¼ '$1'"
        show_usage
        exit 1
        ;;
esac

echo ""
echo "è®­ç»ƒå®Œæˆæ—¶é—´: $(date)"
echo "æ‰€æœ‰æ—¥å¿—ä¿å­˜åœ¨: $LOG_DIR/"

# æ˜¾ç¤ºç£ç›˜ä½¿ç”¨æƒ…å†µ
echo ""
echo "NPUæ¨¡å‹æ–‡ä»¶ç£ç›˜ä½¿ç”¨æƒ…å†µ:"
if [ -d "$OUTPUT_BASE" ]; then
    du -sh "$OUTPUT_BASE"/*npu* 2>/dev/null || echo "æš‚æ— NPUæ¨¡å‹æ–‡ä»¶"
fi

echo ""
echo "NPUè®­ç»ƒè„šæœ¬æ‰§è¡Œå®Œæˆ âœ…"