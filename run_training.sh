#!/bin/bash

# è½¨è¿¹é¢„è®­ç»ƒ+å¾®è°ƒå®Œæ•´è®­ç»ƒè„šæœ¬
# ä½¿ç”¨æ–¹æ³•ï¼š
# 1. åªé¢„è®­ç»ƒï¼š./run_training.sh pretrain
# 2. åªå¾®è°ƒï¼š./run_training.sh finetune /path/to/pretrained/model
# 3. å®Œæ•´æµç¨‹ï¼ˆé¢„è®­ç»ƒ+å¾®è°ƒï¼‰ï¼š./run_training.sh full

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é…ç½®å‚æ•°
BASE_DIR="/home/maomao/project/token_for_nj"
OUTPUT_BASE="../model_out"
PRETRAIN_OUTPUT="$OUTPUT_BASE/trajectory_pretrain_model"
FINETUNE_OUTPUT="$OUTPUT_BASE/trajectory_finetune_model"
LOG_DIR="$OUTPUT_BASE/logs"

# åˆ›å»ºå¿…è¦çš„ç›®å½•
mkdir -p "$LOG_DIR"

# è·å–å½“å‰æ—¶é—´æˆ³
TIMESTAMP=$(date "+%Y%m%d_%H%M%S")

# æ£€æŸ¥CUDAç¯å¢ƒ
if ! nvidia-smi > /dev/null 2>&1; then
    echo "è­¦å‘Š: æœªæ£€æµ‹åˆ°NVIDIA GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰"
fi

# æ£€æŸ¥Pythonç¯å¢ƒ
if ! python -c "import torch, transformers" > /dev/null 2>&1; then
    echo "é”™è¯¯: ç¼ºå°‘å¿…è¦çš„PythonåŒ… (torch, transformers)"
    echo "è¯·å…ˆå®‰è£…: pip install torch transformers"
    exit 1
fi

echo "=== è½¨è¿¹æ¨¡å‹è®­ç»ƒè„šæœ¬ ==="
echo "æ—¶é—´: $(date)"
echo "åŸºç¡€ç›®å½•: $BASE_DIR"
echo "æ—¥å¿—ç›®å½•: $LOG_DIR"

# è¿›å…¥å·¥ä½œç›®å½•
cd "$BASE_DIR"

function run_pretraining() {
    echo ""
    echo "==================="
    echo "å¼€å§‹é¢„è®­ç»ƒé˜¶æ®µ"
    echo "==================="
    
    local log_file="$LOG_DIR/pretrain_$TIMESTAMP.log"
    echo "é¢„è®­ç»ƒæ—¥å¿—: $log_file"
    
    echo "é¢„è®­ç»ƒæ¨¡å‹ä¿å­˜è·¯å¾„: $PRETRAIN_OUTPUT"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šGPUå¯ç”¨
    local gpu_count=$(nvidia-smi --query-gpu=count --format=csv,noheader,nounits 2>/dev/null || echo "0")
    
    if [ "$gpu_count" -gt 1 ]; then
        echo "æ£€æµ‹åˆ° $gpu_count ä¸ªGPUï¼Œä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ"
        
        # åˆ†å¸ƒå¼é¢„è®­ç»ƒ
        python -m torch.distributed.launch \
            --nproc_per_node=$gpu_count \
            --master_port=12345 \
            train_pretrain.py \
            2>&1 | tee "$log_file"
    else
        echo "ä½¿ç”¨å•GPU/CPUè®­ç»ƒ"
        
        # å•å¡é¢„è®­ç»ƒ
        python train_pretrain.py \
            2>&1 | tee "$log_file"
    fi
    
    # æ£€æŸ¥é¢„è®­ç»ƒæ˜¯å¦æˆåŠŸ
    if [ -d "$PRETRAIN_OUTPUT/best_model" ]; then
        echo "âœ… é¢„è®­ç»ƒæˆåŠŸå®Œæˆ"
        echo "é¢„è®­ç»ƒæ¨¡å‹ä¿å­˜åœ¨: $PRETRAIN_OUTPUT/best_model"
        
        # æ˜¾ç¤ºæ¨¡å‹æ–‡ä»¶
        echo "æ¨¡å‹æ–‡ä»¶åˆ—è¡¨:"
        ls -la "$PRETRAIN_OUTPUT/best_model/"
    else
        echo "âŒ é¢„è®­ç»ƒå¤±è´¥ï¼Œæœªæ‰¾åˆ°æ¨¡å‹è¾“å‡º"
        exit 1
    fi
}

function run_finetuning() {
    local pretrained_path="$1"
    
    echo ""
    echo "==================="
    echo "å¼€å§‹å¾®è°ƒé˜¶æ®µ"
    echo "==================="
    
    # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹æ˜¯å¦å­˜åœ¨
    if [ ! -d "$pretrained_path" ]; then
        echo "é”™è¯¯: é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: $pretrained_path"
        echo "è¯·å…ˆè¿è¡Œé¢„è®­ç»ƒæˆ–æä¾›æ­£ç¡®çš„é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„"
        exit 1
    fi
    
    local log_file="$LOG_DIR/finetune_$TIMESTAMP.log"
    echo "å¾®è°ƒæ—¥å¿—: $log_file"
    echo "é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„: $pretrained_path"
    echo "å¾®è°ƒæ¨¡å‹ä¿å­˜è·¯å¾„: $FINETUNE_OUTPUT"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šGPUå¯ç”¨
    local gpu_count=$(nvidia-smi --query-gpu=count --format=csv,noheader,nounits 2>/dev/null || echo "0")
    
    if [ "$gpu_count" -gt 1 ]; then
        echo "æ£€æµ‹åˆ° $gpu_count ä¸ªGPUï¼Œä½¿ç”¨åˆ†å¸ƒå¼å¾®è°ƒ"
        
        # åˆ†å¸ƒå¼å¾®è°ƒ
        python -m torch.distributed.launch \
            --nproc_per_node=$gpu_count \
            --master_port=12346 \
            train_finetune.py \
            --pretrained_model_path "$pretrained_path" \
            2>&1 | tee "$log_file"
    else
        echo "ä½¿ç”¨å•GPU/CPUå¾®è°ƒ"
        
        # å•å¡å¾®è°ƒ
        python train_finetune.py \
            --pretrained_model_path "$pretrained_path" \
            2>&1 | tee "$log_file"
    fi
    
    # æ£€æŸ¥å¾®è°ƒæ˜¯å¦æˆåŠŸ
    if [ -d "$FINETUNE_OUTPUT/best_model" ]; then
        echo "âœ… å¾®è°ƒæˆåŠŸå®Œæˆ"
        echo "æœ€ç»ˆæ¨¡å‹ä¿å­˜åœ¨: $FINETUNE_OUTPUT/best_model"
        
        # æ˜¾ç¤ºæ¨¡å‹æ–‡ä»¶
        echo "æ¨¡å‹æ–‡ä»¶åˆ—è¡¨:"
        ls -la "$FINETUNE_OUTPUT/best_model/"
        
        # å¦‚æœæœ‰éªŒè¯æŠ¥å‘Šï¼Œæ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        if [ -f "$FINETUNE_OUTPUT/best_model/validation_report.json" ]; then
            echo ""
            echo "éªŒè¯ç»“æœæ‘˜è¦:"
            python -c "
import json
with open('$FINETUNE_OUTPUT/best_model/validation_report.json') as f:
    report = json.load(f)
    print(f\"æ•´ä½“å‡†ç¡®ç‡: {report.get('accuracy', 'N/A'):.4f}\")
    print(f\"å®å¹³å‡F1: {report.get('macro avg', {}).get('f1-score', 'N/A'):.4f}\")
    print(f\"åŠ æƒå¹³å‡F1: {report.get('weighted avg', {}).get('f1-score', 'N/A'):.4f}\")
"
        fi
    else
        echo "âŒ å¾®è°ƒå¤±è´¥ï¼Œæœªæ‰¾åˆ°æ¨¡å‹è¾“å‡º"
        exit 1
    fi
}

function show_usage() {
    echo "ç”¨æ³•: $0 <mode> [pretrained_model_path]"
    echo ""
    echo "æ¨¡å¼:"
    echo "  pretrain              - åªè¿è¡Œé¢„è®­ç»ƒ"
    echo "  finetune <model_path> - åªè¿è¡Œå¾®è°ƒï¼Œéœ€è¦æŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„"
    echo "  full                  - è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆé¢„è®­ç»ƒ + å¾®è°ƒï¼‰"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  $0 pretrain"
    echo "  $0 finetune ../model_out/trajectory_pretrain_model/best_model"
    echo "  $0 full"
}

# ä¸»é€»è¾‘
case "$1" in
    "pretrain")
        run_pretraining
        echo ""
        echo "ğŸ‰ é¢„è®­ç»ƒå®Œæˆï¼"
        echo "æ¥ä¸‹æ¥å¯ä»¥è¿è¡Œå¾®è°ƒ: $0 finetune $PRETRAIN_OUTPUT/best_model"
        ;;
    
    "finetune")
        if [ -z "$2" ]; then
            echo "é”™è¯¯: å¾®è°ƒæ¨¡å¼éœ€è¦æŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„"
            show_usage
            exit 1
        fi
        run_finetuning "$2"
        echo ""
        echo "ğŸ‰ å¾®è°ƒå®Œæˆï¼"
        echo "æœ€ç»ˆæ¨¡å‹å¯ç”¨äºæ¨ç†: $FINETUNE_OUTPUT/best_model"
        ;;
    
    "full")
        echo "è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹ï¼šé¢„è®­ç»ƒ -> å¾®è°ƒ"
        
        # 1. é¢„è®­ç»ƒ
        run_pretraining
        
        # 2. å¾®è°ƒ
        run_finetuning "$PRETRAIN_OUTPUT/best_model"
        
        echo ""
        echo "ğŸ‰ğŸ‰ğŸ‰ å®Œæ•´è®­ç»ƒæµç¨‹å®Œæˆï¼"
        echo "é¢„è®­ç»ƒæ¨¡å‹: $PRETRAIN_OUTPUT/best_model"
        echo "æœ€ç»ˆå¾®è°ƒæ¨¡å‹: $FINETUNE_OUTPUT/best_model"
        ;;
    
    *)
        echo "é”™è¯¯: æ— æ•ˆçš„æ¨¡å¼ '$1'"
        show_usage
        exit 1
        ;;
esac

echo ""
echo "è®­ç»ƒå®Œæˆæ—¶é—´: $(date)"
echo "æ‰€æœ‰æ—¥å¿—ä¿å­˜åœ¨: $LOG_DIR/"

# æ˜¾ç¤ºç£ç›˜ä½¿ç”¨æƒ…å†µ
echo ""
echo "æ¨¡å‹æ–‡ä»¶ç£ç›˜ä½¿ç”¨æƒ…å†µ:"
if [ -d "$OUTPUT_BASE" ]; then
    du -sh "$OUTPUT_BASE"/* 2>/dev/null || true
fi