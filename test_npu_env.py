#!/usr/bin/env python3
"""
æ˜‡è…¾NPUè®­ç»ƒç¯å¢ƒå¿«é€Ÿæµ‹è¯•
éªŒè¯æ¨¡å‹æ˜¯å¦èƒ½åœ¨NPUä¸Šæ­£å¸¸è¿è¡Œ
"""

import torch
import torch.nn as nn
import numpy as np

def test_npu_basic():
    """æµ‹è¯•åŸºæœ¬NPUåŠŸèƒ½"""
    print("=== NPUåŸºç¡€åŠŸèƒ½æµ‹è¯• ===")
    
    try:
        import torch_npu
        print("âœ“ torch_npuæ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError:
        print("âœ— torch_npuæ¨¡å—å¯¼å…¥å¤±è´¥")
        return False
    
    # æ£€æŸ¥NPUå¯ç”¨æ€§
    if not torch.npu.is_available():
        print("âœ— NPUä¸å¯ç”¨")
        return False
    
    npu_count = torch.npu.device_count()
    print(f"âœ“ æ£€æµ‹åˆ° {npu_count} ä¸ªNPUè®¾å¤‡")
    
    # æµ‹è¯•NPUè®¾å¤‡ä¿¡æ¯
    for i in range(npu_count):
        device_name = torch.npu.get_device_name(i)
        print(f"  NPU {i}: {device_name}")
    
    return True

def test_npu_operations():
    """æµ‹è¯•NPUå¼ é‡æ“ä½œ"""
    print("\n=== NPUå¼ é‡æ“ä½œæµ‹è¯• ===")
    
    try:
        # åˆ›å»ºNPUå¼ é‡
        device = torch.device('npu:0')
        x = torch.randn(3, 4, device=device)
        y = torch.randn(4, 5, device=device)
        
        print(f"âœ“ NPUå¼ é‡åˆ›å»ºæˆåŠŸ")
        print(f"  x shape: {x.shape}, device: {x.device}")
        print(f"  y shape: {y.shape}, device: {y.device}")
        
        # çŸ©é˜µä¹˜æ³•
        z = torch.mm(x, y)
        print(f"âœ“ NPUçŸ©é˜µä¹˜æ³•æˆåŠŸ: {z.shape}")
        
        # æµ‹è¯•æ¢¯åº¦è®¡ç®—
        x.requires_grad_(True)
        loss = z.sum()
        loss.backward()
        print(f"âœ“ NPUæ¢¯åº¦è®¡ç®—æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âœ— NPUå¼ é‡æ“ä½œå¤±è´¥: {e}")
        return False

def test_npu_model():
    """æµ‹è¯•NPUæ¨¡å‹è®­ç»ƒ"""
    print("\n=== NPUæ¨¡å‹è®­ç»ƒæµ‹è¯• ===")
    
    try:
        device = torch.device('npu:0')
        
        # åˆ›å»ºç®€å•æ¨¡å‹
        class SimpleModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = nn.Linear(10, 1)
                
            def forward(self, x):
                return self.linear(x)
        
        model = SimpleModel().to(device)
        print("âœ“ æ¨¡å‹åˆ›å»ºå¹¶ç§»è‡³NPUæˆåŠŸ")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        try:
            optimizer = torch_npu.optim.NpuFusedAdamW(model.parameters(), lr=0.01)
            print("âœ“ NPUä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ")
        except:
            optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)
            print("âœ“ æ ‡å‡†ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•è®­ç»ƒæ­¥éª¤
        x = torch.randn(32, 10, device=device)
        y = torch.randn(32, 1, device=device)
        
        # å‰å‘ä¼ æ’­
        pred = model(x)
        loss = nn.MSELoss()(pred, y)
        print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸï¼ŒæŸå¤±: {loss.item():.4f}")
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        print("âœ“ åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°æˆåŠŸ")
        
        # æµ‹è¯•æ··åˆç²¾åº¦
        try:
            model_fp16 = model.half()
            x_fp16 = x.half()
            pred_fp16 = model_fp16(x_fp16)
            print("âœ“ æ··åˆç²¾åº¦æ¨ç†æˆåŠŸ")
        except Exception as e:
            print(f"âš  æ··åˆç²¾åº¦æµ‹è¯•å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âœ— NPUæ¨¡å‹è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_npu_distributed():
    """æµ‹è¯•NPUåˆ†å¸ƒå¼é€šä¿¡"""
    print("\n=== NPUåˆ†å¸ƒå¼é€šä¿¡æµ‹è¯• ===")
    
    try:
        import torch.distributed as dist
        
        # æ£€æŸ¥HCCLåç«¯æ”¯æŒ
        if hasattr(dist.Backend, 'HCCL'):
            print("âœ“ HCCLåç«¯æ”¯æŒæ£€æµ‹æˆåŠŸ")
        else:
            print("âš  HCCLåç«¯æœªæ‰¾åˆ°ï¼Œå¯èƒ½ä½¿ç”¨NCCL")
        
        # è¿™é‡Œåªæ˜¯æ£€æµ‹ï¼Œä¸å®é™…åˆå§‹åŒ–åˆ†å¸ƒå¼
        # å› ä¸ºéœ€è¦å¤šè¿›ç¨‹ç¯å¢ƒ
        print("âœ“ åˆ†å¸ƒå¼æ¨¡å—å¯¼å…¥æˆåŠŸ")
        print("  æ³¨æ„: å®Œæ•´çš„åˆ†å¸ƒå¼æµ‹è¯•éœ€è¦å¤šè¿›ç¨‹ç¯å¢ƒ")
        
        return True
        
    except Exception as e:
        print(f"âœ— åˆ†å¸ƒå¼é€šä¿¡æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("æ˜‡è…¾NPUè®­ç»ƒç¯å¢ƒå¿«é€Ÿæµ‹è¯•")
    print("=" * 50)
    
    tests = [
        ("åŸºç¡€åŠŸèƒ½", test_npu_basic),
        ("å¼ é‡æ“ä½œ", test_npu_operations),
        ("æ¨¡å‹è®­ç»ƒ", test_npu_model),
        ("åˆ†å¸ƒå¼æ”¯æŒ", test_npu_distributed),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âœ— {test_name}æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç»“æœæ±‡æ€»:")
    
    passed = 0
    for test_name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"  {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{len(results)} é¡¹æµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼NPUç¯å¢ƒå·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ")
        return 0
    else:
        print("\nâš  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥NPUç¯å¢ƒé…ç½®")
        return 1

if __name__ == "__main__":
    import sys
    sys.exit(main())